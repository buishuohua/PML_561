{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso, Ridge, LassoCV, RidgeCV, LassoLarsIC\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part (a): Function to calculate required sigma^2 for given R^2\n",
    "\n",
    "def calculate_sigma2(X_cov, beta_star, r_square):\n",
    "    \"\"\"\n",
    "    Calculate sigma^2 to achieve desired R^2\n",
    "    Args:\n",
    "        X_cov: design matrix\n",
    "        beta_star: true coefficients\n",
    "        r: desired R^2 value\n",
    "    Returns:\n",
    "        sigma2: required noise variance\n",
    "    \"\"\"\n",
    "\n",
    "    return (1 - r_square) * beta_star.T @ X_cov @ beta_star / r_square\n",
    "\n",
    "# Generate data function\n",
    "\n",
    "def generate_data(n, p, rho, beta_type='sparse', seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic data\n",
    "    Args:\n",
    "        n: sample size\n",
    "        p: dimension\n",
    "        rho: correlation parameter\n",
    "        beta_type: 'sparse' or 'dense'\n",
    "    Returns:\n",
    "        X: design matrix\n",
    "        y: response vector\n",
    "        beta_star: true coefficients\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Generate correlation matrix Σ(ρ)\n",
    "    Sigma = np.zeros((p, p))\n",
    "    for i in range(p):\n",
    "        for j in range(p):\n",
    "            Sigma[i, j] = rho ** abs(i-j)\n",
    "\n",
    "    # Generate X\n",
    "    X = np.random.multivariate_normal(np.zeros(p), Sigma, size=n)\n",
    "\n",
    "    # Generate beta_star based on type\n",
    "    if beta_type == 'sparse':\n",
    "        beta_star = np.array(\n",
    "            [2 / np.sqrt(n) if j < np.sqrt(p) else 0 for j in range(1, p + 1)])\n",
    "    elif beta_type == 'dense':  # dense\n",
    "        beta_star = np.array([5 / j * np.sqrt(n) for j in range(1, p + 1)])\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid beta_type: {beta_type}\")\n",
    "\n",
    "    return X, Sigma, beta_star\n",
    "\n",
    "def ridge_with_ic(X, y, criterion='aic'):\n",
    "    \"\"\"\n",
    "    Ridge regression with AIC or BIC criterion\n",
    "    Args:\n",
    "        X: design matrix (n × p)\n",
    "        y: response vector (n × 1)\n",
    "        criterion: 'aic' or 'bic'\n",
    "    Returns:\n",
    "        fitted Ridge model with best alpha\n",
    "    \"\"\"\n",
    "    n, _ = X.shape\n",
    "    alphas = np.logspace(-6, 6, 20)\n",
    "    best_ic = np.inf\n",
    "    best_alpha = None\n",
    "\n",
    "    for alpha in alphas:\n",
    "        # Fit Ridge with current alpha\n",
    "        ridge = Ridge(alpha=alpha)\n",
    "        ridge.fit(X, y)\n",
    "\n",
    "        # Calculate residual sum of squares\n",
    "        y_pred = ridge.predict(X)\n",
    "        rss = np.sum((y - y_pred) ** 2)\n",
    "\n",
    "        # Calculate effective degrees of freedom\n",
    "        # For Ridge: df = tr(X(X'X + αI)^(-1)X')\n",
    "        svd = np.linalg.svd(X, compute_uv=False)\n",
    "        df = np.sum(svd**2 / (svd**2 + alpha))\n",
    "\n",
    "        # Calculate information criterion\n",
    "        if criterion.lower() == 'aic':\n",
    "            # AIC = n * log(RSS/n) + 2 * df\n",
    "            ic = n * np.log(rss/n) + 2 * df\n",
    "        elif criterion.lower() == 'bic':  # BIC\n",
    "            # BIC = n * log(RSS/n) + log(n) * df\n",
    "            ic = n * np.log(rss/n) + np.log(n) * df\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid criterion: {criterion}\")\n",
    "\n",
    "        # Update best alpha if current IC is lower\n",
    "        if ic < best_ic:\n",
    "            best_ic = ic\n",
    "            best_alpha = alpha\n",
    "\n",
    "    # Fit final model with best alpha\n",
    "    best_model = Ridge(alpha=best_alpha)\n",
    "    best_model.fit(X, y)\n",
    "\n",
    "    # Store the criterion value and best alpha\n",
    "    best_model.criterion_value = best_ic\n",
    "    best_model.best_alpha = best_alpha\n",
    "\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def fit_model(X, y, model_type, tuning_method):\n",
    "    \"\"\"\n",
    "    Fit model with specified tuning method\n",
    "    Args:\n",
    "        X: design matrix\n",
    "        y: response vector\n",
    "        model_type: 'Lasso', 'Ridge', 'Adaptive Lasso', or 'Adaptive Ridge'\n",
    "        tuning_method: 'AIC', 'BIC', or 'LOO-CV'\n",
    "    Returns:\n",
    "        fitted model\n",
    "    \"\"\"\n",
    "    if model_type == 'Lasso':\n",
    "        if tuning_method == 'LOO-CV':\n",
    "            model = LassoCV(cv=len(X))\n",
    "        elif tuning_method in ['AIC', 'BIC']:\n",
    "            model = LassoLarsIC(criterion=tuning_method.lower())\n",
    "        model.fit(X, y)\n",
    "\n",
    "    elif model_type == 'Ridge':\n",
    "        alphas = np.logspace(-6, 6, 20)\n",
    "        if tuning_method == 'LOO-CV':\n",
    "            model = RidgeCV(alphas=alphas, cv=len(X), scoring='neg_mean_squared_error')\n",
    "        elif tuning_method in ['AIC', 'BIC']:\n",
    "            # For Ridge with AIC/BIC, we'll use RidgeCV with cross-validation\n",
    "            model = ridge_with_ic(X, y, criterion=tuning_method)\n",
    "        model.fit(X, y)\n",
    "\n",
    "    elif model_type == 'Adaptive Lasso':\n",
    "        # First fit Ridge to get initial weights\n",
    "        lasso = Lasso(alpha=1.0)\n",
    "        lasso.fit(X, y)\n",
    "        weights = 1 / np.abs(lasso.coef_ + 1e-6)\n",
    "        weights = weights / np.mean(weights)\n",
    "        X_weighted = X * weights\n",
    "\n",
    "        if tuning_method == 'LOO-CV':\n",
    "            model = LassoCV(cv=len(X))\n",
    "        elif tuning_method in ['AIC', 'BIC']:\n",
    "            model = LassoLarsIC(criterion=tuning_method.lower())\n",
    "\n",
    "        model.fit(X_weighted, y)\n",
    "        # Store weights for prediction\n",
    "        model.weights = weights\n",
    "\n",
    "    elif model_type == 'Adaptive Ridge':\n",
    "        # First fit OLS to get initial weights\n",
    "        ridge = Ridge(alpha=1.0)\n",
    "        ridge.fit(X, y)\n",
    "        weights = 1 / np.abs(ridge.coef_ + 1e-6)\n",
    "        weights = weights / np.mean(weights)\n",
    "        X_weighted = X * weights\n",
    "\n",
    "        alphas = np.logspace(-6, 6, 20)\n",
    "        if tuning_method == 'LOO-CV':\n",
    "            model = RidgeCV(alphas=alphas, cv=len(X), scoring='neg_mean_squared_error')\n",
    "        elif tuning_method in ['AIC', 'BIC']:\n",
    "            model = ridge_with_ic(X, y, criterion=tuning_method)\n",
    "\n",
    "        model.fit(X_weighted, y)\n",
    "        # Store weights for prediction\n",
    "        model.weights = weights\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model_type: {model_type}\")\n",
    "\n",
    "    # Add predict method for adaptive models\n",
    "    if model_type in ['Adaptive Lasso', 'Adaptive Ridge']:\n",
    "        original_predict = model.predict\n",
    "        def new_predict(self, X_new):\n",
    "            return original_predict(X_new * model.weights)\n",
    "        model.predict = new_predict.__get__(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "def run_simulation(n, p, rho, beta_type, n_datasets=1000):\n",
    "    \"\"\"\n",
    "    Run full simulation for given parameters\n",
    "    \"\"\"\n",
    "    def single_simulation(sim_id):\n",
    "        # Generate data\n",
    "        X, X_cov, beta_star = generate_data(n, p, rho, beta_type, seed=sim_id)\n",
    "\n",
    "        # Calculate required sigma^2 for R^2 = 0.8\n",
    "        sigma2 = calculate_sigma2(X_cov, beta_star, 0.8)\n",
    "\n",
    "        # Generate response\n",
    "        epsilon = np.random.normal(0, np.sqrt(sigma2), n)\n",
    "        y = X @ beta_star + epsilon\n",
    "\n",
    "        results = []\n",
    "        models = [\n",
    "            (i, j)\n",
    "            for i in ['Lasso', 'Ridge']  # Reduced model types\n",
    "            for j in ['AIC', 'BIC', 'LOO-CV']\n",
    "        ]\n",
    "\n",
    "        for model_type, tuning in models:\n",
    "            try:\n",
    "                model = fit_model(X, y, model_type, tuning)\n",
    "                mse = mean_squared_error(y, model.predict(X))\n",
    "                results.append({\n",
    "                    'model': model_type,\n",
    "                    'tuning': tuning,\n",
    "                    'mse': mse,\n",
    "                    'n': n,\n",
    "                    'p': p,\n",
    "                    'rho': rho,\n",
    "                    'beta_type': beta_type\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error in simulation {sim_id} with {model_type}, {tuning}: {str(e)}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    # Run simulations in parallel\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(single_simulation)(i)\n",
    "        for i in tqdm(range(n_datasets))\n",
    "    )\n",
    "\n",
    "    # Flatten results list\n",
    "    flat_results = [item for sublist in results for item in sublist]\n",
    "    return pd.DataFrame(flat_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 176/1000 [00:14<01:16, 10.80it/s]"
     ]
    }
   ],
   "source": [
    "# Run simulations for all parameter combinations\n",
    "parameter_settings = [\n",
    "    {'n': 100, 'p': p, 'rho': rho, 'beta_type': beta_type}\n",
    "    for p in [10, 25, 50]\n",
    "    for rho in [0, 0.25, 0.5]\n",
    "    for beta_type in ['sparse', 'dense']\n",
    "]\n",
    "\n",
    "# Store results\n",
    "all_results = []\n",
    "for params in parameter_settings:\n",
    "    results = run_simulation(**params)\n",
    "    all_results.append(results)\n",
    "\n",
    "# Combine and analyze results\n",
    "final_results = pd.concat(all_results, axis=0).reset_index(drop=True)\n",
    "print(final_results)\n",
    "\n",
    "average_results = (final_results\n",
    "    .groupby(['model', 'tuning', 'p', 'rho', 'beta_type'])['mse']\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Create a more readable table format\n",
    "table = average_results.pivot_table(\n",
    "    index=['p', 'rho', 'beta_type'],\n",
    "    columns=['model', 'tuning'],\n",
    "    values='mse'\n",
    ")\n",
    "\n",
    "# Print the formatted table\n",
    "print(\"\\nAverage MSE across 1000 simulations:\")\n",
    "print(table)\n",
    "\n",
    "# Optionally, save to CSV\n",
    "# table.to_csv('simulation_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
